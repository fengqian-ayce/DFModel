dataflow_graph {
  kernels {
    name: "MLP_1"
    id: 1
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_2"
    id: 2
    topological_number: 1
    config: 1
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_3"
    id: 3
    topological_number: 2
    config: 2
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_4"
    id: 4
    topological_number: 3
    config: 3
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_5"
    id: 5
    topological_number: 4
    config: 4
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_6"
    id: 6
    topological_number: 5
    config: 5
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_7"
    id: 7
    topological_number: 6
    config: 6
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_8"
    id: 8
    topological_number: 7
    config: 7
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_9"
    id: 9
    topological_number: 8
    config: 8
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_10"
    id: 10
    topological_number: 9
    config: 9
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_11"
    id: 11
    topological_number: 10
    config: 10
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_12"
    id: 12
    topological_number: 11
    config: 11
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_13"
    id: 13
    topological_number: 12
    config: 12
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      communication_type: ALL_TO_ALL
      communication_size: 10304000.0
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
      memory_size: 1318912000.0
    }
  }
  kernels {
    name: "MLP_14"
    id: 14
    topological_number: 13
    config: 13
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_15"
    id: 15
    topological_number: 14
    config: 14
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_16"
    id: 16
    topological_number: 15
    config: 15
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_17"
    id: 17
    topological_number: 16
    config: 16
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_18"
    id: 18
    topological_number: 17
    config: 17
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_19"
    id: 19
    topological_number: 18
    config: 18
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_20"
    id: 20
    topological_number: 19
    config: 19
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_21"
    id: 21
    topological_number: 20
    config: 20
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_22"
    id: 22
    topological_number: 21
    config: 21
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_23"
    id: 23
    topological_number: 22
    config: 22
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_24"
    id: 24
    topological_number: 23
    config: 23
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_25"
    id: 25
    topological_number: 24
    config: 24
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_26"
    id: 26
    topological_number: 25
    config: 25
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_27"
    id: 27
    topological_number: 26
    config: 26
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_28"
    id: 28
    topological_number: 27
    config: 27
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_29"
    id: 29
    topological_number: 28
    config: 28
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_30"
    id: 30
    topological_number: 29
    config: 29
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_31"
    id: 31
    topological_number: 30
    config: 30
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_32"
    id: 32
    topological_number: 31
    config: 31
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_33"
    id: 33
    topological_number: 32
    config: 32
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_34"
    id: 34
    topological_number: 33
    config: 33
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_35"
    id: 35
    topological_number: 34
    config: 34
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_36"
    id: 36
    topological_number: 35
    config: 35
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_37"
    id: 37
    topological_number: 36
    config: 36
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_38"
    id: 38
    topological_number: 37
    config: 37
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_39"
    id: 39
    topological_number: 38
    config: 38
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_40"
    id: 40
    topological_number: 39
    config: 39
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_41"
    id: 41
    topological_number: 40
    config: 40
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_42"
    id: 42
    topological_number: 41
    config: 41
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_43"
    id: 43
    topological_number: 42
    config: 42
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_43_bwd"
    id: 44
    config: 43
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_42_bwd"
    id: 45
    topological_number: 1
    config: 44
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_41_bwd"
    id: 46
    topological_number: 2
    config: 45
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_40_bwd"
    id: 47
    topological_number: 3
    config: 46
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_39_bwd"
    id: 48
    topological_number: 4
    config: 47
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_38_bwd"
    id: 49
    topological_number: 5
    config: 48
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_37_bwd"
    id: 50
    topological_number: 6
    config: 49
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_36_bwd"
    id: 51
    topological_number: 7
    config: 50
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_35_bwd"
    id: 52
    topological_number: 8
    config: 51
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_34_bwd"
    id: 53
    topological_number: 9
    config: 52
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_33_bwd"
    id: 54
    topological_number: 10
    config: 53
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_32_bwd"
    id: 55
    topological_number: 11
    config: 54
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_31_bwd"
    id: 56
    topological_number: 12
    config: 55
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_30_bwd"
    id: 57
    topological_number: 13
    config: 56
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_29_bwd"
    id: 58
    topological_number: 14
    config: 57
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_28_bwd"
    id: 59
    topological_number: 15
    config: 58
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_27_bwd"
    id: 60
    topological_number: 16
    config: 59
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_26_bwd"
    id: 61
    topological_number: 17
    config: 60
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_25_bwd"
    id: 62
    topological_number: 18
    config: 61
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_24_bwd"
    id: 63
    topological_number: 19
    config: 62
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_23_bwd"
    id: 64
    topological_number: 20
    config: 63
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_22_bwd"
    id: 65
    topological_number: 21
    config: 64
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_21_bwd"
    id: 66
    topological_number: 22
    config: 65
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_20_bwd"
    id: 67
    topological_number: 23
    config: 66
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_19_bwd"
    id: 68
    topological_number: 24
    config: 67
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_18_bwd"
    id: 69
    topological_number: 25
    config: 68
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_17_bwd"
    id: 70
    topological_number: 26
    config: 69
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_16_bwd"
    id: 71
    topological_number: 27
    config: 70
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_15_bwd"
    id: 72
    topological_number: 28
    config: 71
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_14_bwd"
    id: 73
    topological_number: 29
    config: 72
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_13_bwd"
    id: 74
    topological_number: 30
    config: 73
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      communication_type: ALL_TO_ALL
      communication_size: 10304000.0
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
      memory_size: 1318912000.0
    }
  }
  kernels {
    name: "MLP_12_bwd"
    id: 75
    topological_number: 31
    config: 74
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_11_bwd"
    id: 76
    topological_number: 32
    config: 75
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_10_bwd"
    id: 77
    topological_number: 33
    config: 76
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_9_bwd"
    id: 78
    topological_number: 34
    config: 77
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_8_bwd"
    id: 79
    topological_number: 35
    config: 78
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_7_bwd"
    id: 80
    topological_number: 36
    config: 79
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_6_bwd"
    id: 81
    topological_number: 37
    config: 80
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_5_bwd"
    id: 82
    topological_number: 38
    config: 81
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_4_bwd"
    id: 83
    topological_number: 39
    config: 82
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_3_bwd"
    id: 84
    topological_number: 40
    config: 83
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_2_bwd"
    id: 85
    topological_number: 41
    config: 84
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_1_bwd"
    id: 86
    topological_number: 42
    config: 85
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 682
      K: 682
      N: 2048
      input_tensor_size: 2793472.0
      weight_tensor_size: 930248.0
      output_tensor_size: 2793472.0
      sharding: NO_SHARDING
      shard_outer_M: 682
      shard_K: 682
      shard_N: 2048
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_43_bwd_weight_update"
    id: 87
    topological_number: 42
    config: 86
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 682
      K: 2048
      N: 682
      input_tensor_1_size: 2793472.0
      input_tensor_2_size: 2793472.0
      output_tensor_size: 930248.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 930248.0
      shard_outer_M: 682
      shard_K: 2048
      shard_N: 682
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_42_bwd_weight_update"
    id: 88
    topological_number: 41
    config: 87
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 682
      K: 2048
      N: 682
      input_tensor_1_size: 2793472.0
      input_tensor_2_size: 2793472.0
      output_tensor_size: 930248.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 930248.0
      shard_outer_M: 682
      shard_K: 2048
      shard_N: 682
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_41_bwd_weight_update"
    id: 89
    topological_number: 40
    config: 88
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 682
      K: 2048
      N: 682
      input_tensor_1_size: 2793472.0
      input_tensor_2_size: 2793472.0
      output_tensor_size: 930248.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 930248.0
      shard_outer_M: 682
      shard_K: 2048
      shard_N: 682
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_40_bwd_weight_update"
    id: 90
    topological_number: 39
    config: 89
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 682
      K: 2048
      N: 682
      input_tensor_1_size: 2793472.0
      input_tensor_2_size: 2793472.0
      output_tensor_size: 930248.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 930248.0
      shard_outer_M: 682
      shard_K: 2048
      shard_N: 682
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_39_bwd_weight_update"
    id: 91
    topological_number: 38
    config: 90
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 682
      K: 2048
      N: 682
      input_tensor_1_size: 2793472.0
      input_tensor_2_size: 2793472.0
      output_tensor_size: 930248.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 930248.0
      shard_outer_M: 682
      shard_K: 2048
      shard_N: 682
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_38_bwd_weight_update"
    id: 92
    topological_number: 37
    config: 91
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 682
      K: 2048
      N: 682
      input_tensor_1_size: 2793472.0
      input_tensor_2_size: 2793472.0
      output_tensor_size: 930248.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 930248.0
      shard_outer_M: 682
      shard_K: 2048
      shard_N: 682
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_37_bwd_weight_update"
    id: 93
    topological_number: 36
    config: 92
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 682
      K: 2048
      N: 682
      input_tensor_1_size: 2793472.0
      input_tensor_2_size: 2793472.0
      output_tensor_size: 930248.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 930248.0
      shard_outer_M: 682
      shard_K: 2048
      shard_N: 682
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_36_bwd_weight_update"
    id: 94
    topological_number: 35
    config: 93
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 682
      K: 2048
      N: 682
      input_tensor_1_size: 2793472.0
      input_tensor_2_size: 2793472.0
      output_tensor_size: 930248.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 930248.0
      shard_outer_M: 682
      shard_K: 2048
      shard_N: 682
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_35_bwd_weight_update"
    id: 95
    topological_number: 34
    config: 94
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 682
      K: 2048
      N: 682
      input_tensor_1_size: 2793472.0
      input_tensor_2_size: 2793472.0
      output_tensor_size: 930248.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 930248.0
      shard_outer_M: 682
      shard_K: 2048
      shard_N: 682
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_34_bwd_weight_update"
    id: 96
    topological_number: 33
    config: 95
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 682
      K: 2048
      N: 682
      input_tensor_1_size: 2793472.0
      input_tensor_2_size: 2793472.0
      output_tensor_size: 930248.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 930248.0
      shard_outer_M: 682
      shard_K: 2048
      shard_N: 682
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_33_bwd_weight_update"
    id: 97
    topological_number: 32
    config: 96
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 682
      K: 2048
      N: 682
      input_tensor_1_size: 2793472.0
      input_tensor_2_size: 2793472.0
      output_tensor_size: 930248.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 930248.0
      shard_outer_M: 682
      shard_K: 2048
      shard_N: 682
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_32_bwd_weight_update"
    id: 98
    topological_number: 31
    config: 97
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 682
      K: 2048
      N: 682
      input_tensor_1_size: 2793472.0
      input_tensor_2_size: 2793472.0
      output_tensor_size: 930248.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 930248.0
      shard_outer_M: 682
      shard_K: 2048
      shard_N: 682
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_31_bwd_weight_update"
    id: 99
    topological_number: 30
    config: 98
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 682
      K: 2048
      N: 682
      input_tensor_1_size: 2793472.0
      input_tensor_2_size: 2793472.0
      output_tensor_size: 930248.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 930248.0
      shard_outer_M: 682
      shard_K: 2048
      shard_N: 682
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_30_bwd_weight_update"
    id: 100
    topological_number: 29
    config: 99
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 682
      K: 2048
      N: 682
      input_tensor_1_size: 2793472.0
      input_tensor_2_size: 2793472.0
      output_tensor_size: 930248.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 930248.0
      shard_outer_M: 682
      shard_K: 2048
      shard_N: 682
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_29_bwd_weight_update"
    id: 101
    topological_number: 28
    config: 100
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 682
      K: 2048
      N: 682
      input_tensor_1_size: 2793472.0
      input_tensor_2_size: 2793472.0
      output_tensor_size: 930248.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 930248.0
      shard_outer_M: 682
      shard_K: 2048
      shard_N: 682
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_28_bwd_weight_update"
    id: 102
    topological_number: 27
    config: 101
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 682
      K: 2048
      N: 682
      input_tensor_1_size: 2793472.0
      input_tensor_2_size: 2793472.0
      output_tensor_size: 930248.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 930248.0
      shard_outer_M: 682
      shard_K: 2048
      shard_N: 682
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_27_bwd_weight_update"
    id: 103
    topological_number: 26
    config: 102
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 682
      K: 2048
      N: 682
      input_tensor_1_size: 2793472.0
      input_tensor_2_size: 2793472.0
      output_tensor_size: 930248.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 930248.0
      shard_outer_M: 682
      shard_K: 2048
      shard_N: 682
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_26_bwd_weight_update"
    id: 104
    topological_number: 25
    config: 103
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 682
      K: 2048
      N: 682
      input_tensor_1_size: 2793472.0
      input_tensor_2_size: 2793472.0
      output_tensor_size: 930248.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 930248.0
      shard_outer_M: 682
      shard_K: 2048
      shard_N: 682
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_25_bwd_weight_update"
    id: 105
    topological_number: 24
    config: 104
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 682
      K: 2048
      N: 682
      input_tensor_1_size: 2793472.0
      input_tensor_2_size: 2793472.0
      output_tensor_size: 930248.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 930248.0
      shard_outer_M: 682
      shard_K: 2048
      shard_N: 682
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_24_bwd_weight_update"
    id: 106
    topological_number: 23
    config: 105
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 682
      K: 2048
      N: 682
      input_tensor_1_size: 2793472.0
      input_tensor_2_size: 2793472.0
      output_tensor_size: 930248.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 930248.0
      shard_outer_M: 682
      shard_K: 2048
      shard_N: 682
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_23_bwd_weight_update"
    id: 107
    topological_number: 22
    config: 106
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 682
      K: 2048
      N: 682
      input_tensor_1_size: 2793472.0
      input_tensor_2_size: 2793472.0
      output_tensor_size: 930248.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 930248.0
      shard_outer_M: 682
      shard_K: 2048
      shard_N: 682
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_22_bwd_weight_update"
    id: 108
    topological_number: 21
    config: 107
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 682
      K: 2048
      N: 682
      input_tensor_1_size: 2793472.0
      input_tensor_2_size: 2793472.0
      output_tensor_size: 930248.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 930248.0
      shard_outer_M: 682
      shard_K: 2048
      shard_N: 682
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_21_bwd_weight_update"
    id: 109
    topological_number: 22
    config: 108
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 682
      K: 2048
      N: 682
      input_tensor_1_size: 2793472.0
      input_tensor_2_size: 2793472.0
      output_tensor_size: 930248.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 930248.0
      shard_outer_M: 682
      shard_K: 2048
      shard_N: 682
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_20_bwd_weight_update"
    id: 110
    topological_number: 23
    config: 109
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 682
      K: 2048
      N: 682
      input_tensor_1_size: 2793472.0
      input_tensor_2_size: 2793472.0
      output_tensor_size: 930248.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 930248.0
      shard_outer_M: 682
      shard_K: 2048
      shard_N: 682
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_19_bwd_weight_update"
    id: 111
    topological_number: 24
    config: 110
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 682
      K: 2048
      N: 682
      input_tensor_1_size: 2793472.0
      input_tensor_2_size: 2793472.0
      output_tensor_size: 930248.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 930248.0
      shard_outer_M: 682
      shard_K: 2048
      shard_N: 682
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_18_bwd_weight_update"
    id: 112
    topological_number: 25
    config: 111
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 682
      K: 2048
      N: 682
      input_tensor_1_size: 2793472.0
      input_tensor_2_size: 2793472.0
      output_tensor_size: 930248.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 930248.0
      shard_outer_M: 682
      shard_K: 2048
      shard_N: 682
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_17_bwd_weight_update"
    id: 113
    topological_number: 26
    config: 112
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 682
      K: 2048
      N: 682
      input_tensor_1_size: 2793472.0
      input_tensor_2_size: 2793472.0
      output_tensor_size: 930248.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 930248.0
      shard_outer_M: 682
      shard_K: 2048
      shard_N: 682
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_16_bwd_weight_update"
    id: 114
    topological_number: 27
    config: 113
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 682
      K: 2048
      N: 682
      input_tensor_1_size: 2793472.0
      input_tensor_2_size: 2793472.0
      output_tensor_size: 930248.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 930248.0
      shard_outer_M: 682
      shard_K: 2048
      shard_N: 682
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_15_bwd_weight_update"
    id: 115
    topological_number: 28
    config: 114
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 682
      K: 2048
      N: 682
      input_tensor_1_size: 2793472.0
      input_tensor_2_size: 2793472.0
      output_tensor_size: 930248.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 930248.0
      shard_outer_M: 682
      shard_K: 2048
      shard_N: 682
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_14_bwd_weight_update"
    id: 116
    topological_number: 29
    config: 115
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 682
      K: 2048
      N: 682
      input_tensor_1_size: 2793472.0
      input_tensor_2_size: 2793472.0
      output_tensor_size: 930248.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 930248.0
      shard_outer_M: 682
      shard_K: 2048
      shard_N: 682
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_13_bwd_weight_update"
    id: 117
    topological_number: 30
    config: 116
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 682
      K: 2048
      N: 682
      input_tensor_1_size: 2793472.0
      input_tensor_2_size: 2793472.0
      output_tensor_size: 930248.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 930248.0
      shard_outer_M: 682
      shard_K: 2048
      shard_N: 682
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_12_bwd_weight_update"
    id: 118
    topological_number: 31
    config: 117
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 682
      K: 2048
      N: 682
      input_tensor_1_size: 2793472.0
      input_tensor_2_size: 2793472.0
      output_tensor_size: 930248.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 930248.0
      shard_outer_M: 682
      shard_K: 2048
      shard_N: 682
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_11_bwd_weight_update"
    id: 119
    topological_number: 32
    config: 118
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 682
      K: 2048
      N: 682
      input_tensor_1_size: 2793472.0
      input_tensor_2_size: 2793472.0
      output_tensor_size: 930248.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 930248.0
      shard_outer_M: 682
      shard_K: 2048
      shard_N: 682
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_10_bwd_weight_update"
    id: 120
    topological_number: 33
    config: 119
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 682
      K: 2048
      N: 682
      input_tensor_1_size: 2793472.0
      input_tensor_2_size: 2793472.0
      output_tensor_size: 930248.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 930248.0
      shard_outer_M: 682
      shard_K: 2048
      shard_N: 682
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_9_bwd_weight_update"
    id: 121
    topological_number: 34
    config: 120
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 682
      K: 2048
      N: 682
      input_tensor_1_size: 2793472.0
      input_tensor_2_size: 2793472.0
      output_tensor_size: 930248.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 930248.0
      shard_outer_M: 682
      shard_K: 2048
      shard_N: 682
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_8_bwd_weight_update"
    id: 122
    topological_number: 35
    config: 121
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 682
      K: 2048
      N: 682
      input_tensor_1_size: 2793472.0
      input_tensor_2_size: 2793472.0
      output_tensor_size: 930248.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 930248.0
      shard_outer_M: 682
      shard_K: 2048
      shard_N: 682
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_7_bwd_weight_update"
    id: 123
    topological_number: 36
    config: 122
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 682
      K: 2048
      N: 682
      input_tensor_1_size: 2793472.0
      input_tensor_2_size: 2793472.0
      output_tensor_size: 930248.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 930248.0
      shard_outer_M: 682
      shard_K: 2048
      shard_N: 682
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_6_bwd_weight_update"
    id: 124
    topological_number: 37
    config: 123
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 682
      K: 2048
      N: 682
      input_tensor_1_size: 2793472.0
      input_tensor_2_size: 2793472.0
      output_tensor_size: 930248.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 930248.0
      shard_outer_M: 682
      shard_K: 2048
      shard_N: 682
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_5_bwd_weight_update"
    id: 125
    topological_number: 38
    config: 124
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 682
      K: 2048
      N: 682
      input_tensor_1_size: 2793472.0
      input_tensor_2_size: 2793472.0
      output_tensor_size: 930248.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 930248.0
      shard_outer_M: 682
      shard_K: 2048
      shard_N: 682
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_4_bwd_weight_update"
    id: 126
    topological_number: 39
    config: 125
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 682
      K: 2048
      N: 682
      input_tensor_1_size: 2793472.0
      input_tensor_2_size: 2793472.0
      output_tensor_size: 930248.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 930248.0
      shard_outer_M: 682
      shard_K: 2048
      shard_N: 682
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_3_bwd_weight_update"
    id: 127
    topological_number: 40
    config: 126
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 682
      K: 2048
      N: 682
      input_tensor_1_size: 2793472.0
      input_tensor_2_size: 2793472.0
      output_tensor_size: 930248.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 930248.0
      shard_outer_M: 682
      shard_K: 2048
      shard_N: 682
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_2_bwd_weight_update"
    id: 128
    topological_number: 41
    config: 127
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 682
      K: 2048
      N: 682
      input_tensor_1_size: 2793472.0
      input_tensor_2_size: 2793472.0
      output_tensor_size: 930248.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 930248.0
      shard_outer_M: 682
      shard_K: 2048
      shard_N: 682
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_1_bwd_weight_update"
    id: 129
    topological_number: 42
    config: 128
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 682
      K: 2048
      N: 682
      input_tensor_1_size: 2793472.0
      input_tensor_2_size: 2793472.0
      output_tensor_size: 930248.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 930248.0
      shard_outer_M: 682
      shard_K: 2048
      shard_N: 682
      tiling: NO_TILING
    }
  }
  connections {
    startIdx: 1
    endIdx: 2
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 1
    startName: "MLP_1"
    endName: "MLP_2"
  }
  connections {
    startIdx: 2
    endIdx: 3
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 2
    startName: "MLP_2"
    endName: "MLP_3"
  }
  connections {
    startIdx: 3
    endIdx: 4
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 3
    startName: "MLP_3"
    endName: "MLP_4"
  }
  connections {
    startIdx: 4
    endIdx: 5
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 4
    startName: "MLP_4"
    endName: "MLP_5"
  }
  connections {
    startIdx: 5
    endIdx: 6
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 5
    startName: "MLP_5"
    endName: "MLP_6"
  }
  connections {
    startIdx: 6
    endIdx: 7
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 6
    startName: "MLP_6"
    endName: "MLP_7"
  }
  connections {
    startIdx: 7
    endIdx: 8
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 7
    startName: "MLP_7"
    endName: "MLP_8"
  }
  connections {
    startIdx: 8
    endIdx: 9
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 8
    startName: "MLP_8"
    endName: "MLP_9"
  }
  connections {
    startIdx: 9
    endIdx: 10
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 9
    startName: "MLP_9"
    endName: "MLP_10"
  }
  connections {
    startIdx: 10
    endIdx: 11
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 10
    startName: "MLP_10"
    endName: "MLP_11"
  }
  connections {
    startIdx: 11
    endIdx: 12
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 11
    startName: "MLP_11"
    endName: "MLP_12"
  }
  connections {
    startIdx: 12
    endIdx: 13
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 12
    startName: "MLP_12"
    endName: "MLP_13"
  }
  connections {
    startIdx: 13
    endIdx: 14
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 13
    startName: "MLP_13"
    endName: "MLP_14"
  }
  connections {
    startIdx: 14
    endIdx: 15
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 14
    startName: "MLP_14"
    endName: "MLP_15"
  }
  connections {
    startIdx: 15
    endIdx: 16
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 15
    startName: "MLP_15"
    endName: "MLP_16"
  }
  connections {
    startIdx: 16
    endIdx: 17
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 16
    startName: "MLP_16"
    endName: "MLP_17"
  }
  connections {
    startIdx: 17
    endIdx: 18
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 17
    startName: "MLP_17"
    endName: "MLP_18"
  }
  connections {
    startIdx: 18
    endIdx: 19
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 18
    startName: "MLP_18"
    endName: "MLP_19"
  }
  connections {
    startIdx: 19
    endIdx: 20
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 19
    startName: "MLP_19"
    endName: "MLP_20"
  }
  connections {
    startIdx: 20
    endIdx: 21
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 20
    startName: "MLP_20"
    endName: "MLP_21"
  }
  connections {
    startIdx: 21
    endIdx: 22
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 21
    startName: "MLP_21"
    endName: "MLP_22"
  }
  connections {
    startIdx: 22
    endIdx: 23
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 22
    startName: "MLP_22"
    endName: "MLP_23"
  }
  connections {
    startIdx: 23
    endIdx: 24
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 23
    startName: "MLP_23"
    endName: "MLP_24"
  }
  connections {
    startIdx: 24
    endIdx: 25
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 24
    startName: "MLP_24"
    endName: "MLP_25"
  }
  connections {
    startIdx: 25
    endIdx: 26
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 25
    startName: "MLP_25"
    endName: "MLP_26"
  }
  connections {
    startIdx: 26
    endIdx: 27
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 26
    startName: "MLP_26"
    endName: "MLP_27"
  }
  connections {
    startIdx: 27
    endIdx: 28
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 27
    startName: "MLP_27"
    endName: "MLP_28"
  }
  connections {
    startIdx: 28
    endIdx: 29
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 28
    startName: "MLP_28"
    endName: "MLP_29"
  }
  connections {
    startIdx: 29
    endIdx: 30
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 29
    startName: "MLP_29"
    endName: "MLP_30"
  }
  connections {
    startIdx: 30
    endIdx: 31
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 30
    startName: "MLP_30"
    endName: "MLP_31"
  }
  connections {
    startIdx: 31
    endIdx: 32
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 31
    startName: "MLP_31"
    endName: "MLP_32"
  }
  connections {
    startIdx: 32
    endIdx: 33
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 32
    startName: "MLP_32"
    endName: "MLP_33"
  }
  connections {
    startIdx: 33
    endIdx: 34
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 33
    startName: "MLP_33"
    endName: "MLP_34"
  }
  connections {
    startIdx: 34
    endIdx: 35
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 34
    startName: "MLP_34"
    endName: "MLP_35"
  }
  connections {
    startIdx: 35
    endIdx: 36
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 35
    startName: "MLP_35"
    endName: "MLP_36"
  }
  connections {
    startIdx: 36
    endIdx: 37
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 36
    startName: "MLP_36"
    endName: "MLP_37"
  }
  connections {
    startIdx: 37
    endIdx: 38
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 37
    startName: "MLP_37"
    endName: "MLP_38"
  }
  connections {
    startIdx: 38
    endIdx: 39
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 38
    startName: "MLP_38"
    endName: "MLP_39"
  }
  connections {
    startIdx: 39
    endIdx: 40
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 39
    startName: "MLP_39"
    endName: "MLP_40"
  }
  connections {
    startIdx: 40
    endIdx: 41
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 40
    startName: "MLP_40"
    endName: "MLP_41"
  }
  connections {
    startIdx: 41
    endIdx: 42
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 41
    startName: "MLP_41"
    endName: "MLP_42"
  }
  connections {
    startIdx: 42
    endIdx: 43
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 42
    startName: "MLP_42"
    endName: "MLP_43"
  }
  connections {
    startIdx: 44
    endIdx: 45
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 43
    startName: "MLP_43_bwd"
    endName: "MLP_42_bwd"
  }
  connections {
    startIdx: 45
    endIdx: 46
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 44
    startName: "MLP_42_bwd"
    endName: "MLP_41_bwd"
  }
  connections {
    startIdx: 46
    endIdx: 47
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 45
    startName: "MLP_41_bwd"
    endName: "MLP_40_bwd"
  }
  connections {
    startIdx: 47
    endIdx: 48
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 46
    startName: "MLP_40_bwd"
    endName: "MLP_39_bwd"
  }
  connections {
    startIdx: 48
    endIdx: 49
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 47
    startName: "MLP_39_bwd"
    endName: "MLP_38_bwd"
  }
  connections {
    startIdx: 49
    endIdx: 50
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 48
    startName: "MLP_38_bwd"
    endName: "MLP_37_bwd"
  }
  connections {
    startIdx: 50
    endIdx: 51
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 49
    startName: "MLP_37_bwd"
    endName: "MLP_36_bwd"
  }
  connections {
    startIdx: 51
    endIdx: 52
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 50
    startName: "MLP_36_bwd"
    endName: "MLP_35_bwd"
  }
  connections {
    startIdx: 52
    endIdx: 53
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 51
    startName: "MLP_35_bwd"
    endName: "MLP_34_bwd"
  }
  connections {
    startIdx: 53
    endIdx: 54
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 52
    startName: "MLP_34_bwd"
    endName: "MLP_33_bwd"
  }
  connections {
    startIdx: 54
    endIdx: 55
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 53
    startName: "MLP_33_bwd"
    endName: "MLP_32_bwd"
  }
  connections {
    startIdx: 55
    endIdx: 56
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 54
    startName: "MLP_32_bwd"
    endName: "MLP_31_bwd"
  }
  connections {
    startIdx: 56
    endIdx: 57
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 55
    startName: "MLP_31_bwd"
    endName: "MLP_30_bwd"
  }
  connections {
    startIdx: 57
    endIdx: 58
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 56
    startName: "MLP_30_bwd"
    endName: "MLP_29_bwd"
  }
  connections {
    startIdx: 58
    endIdx: 59
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 57
    startName: "MLP_29_bwd"
    endName: "MLP_28_bwd"
  }
  connections {
    startIdx: 59
    endIdx: 60
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 58
    startName: "MLP_28_bwd"
    endName: "MLP_27_bwd"
  }
  connections {
    startIdx: 60
    endIdx: 61
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 59
    startName: "MLP_27_bwd"
    endName: "MLP_26_bwd"
  }
  connections {
    startIdx: 61
    endIdx: 62
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 60
    startName: "MLP_26_bwd"
    endName: "MLP_25_bwd"
  }
  connections {
    startIdx: 62
    endIdx: 63
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 61
    startName: "MLP_25_bwd"
    endName: "MLP_24_bwd"
  }
  connections {
    startIdx: 63
    endIdx: 64
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 62
    startName: "MLP_24_bwd"
    endName: "MLP_23_bwd"
  }
  connections {
    startIdx: 64
    endIdx: 65
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 63
    startName: "MLP_23_bwd"
    endName: "MLP_22_bwd"
  }
  connections {
    startIdx: 65
    endIdx: 66
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 64
    startName: "MLP_22_bwd"
    endName: "MLP_21_bwd"
  }
  connections {
    startIdx: 66
    endIdx: 67
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 65
    startName: "MLP_21_bwd"
    endName: "MLP_20_bwd"
  }
  connections {
    startIdx: 67
    endIdx: 68
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 66
    startName: "MLP_20_bwd"
    endName: "MLP_19_bwd"
  }
  connections {
    startIdx: 68
    endIdx: 69
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 67
    startName: "MLP_19_bwd"
    endName: "MLP_18_bwd"
  }
  connections {
    startIdx: 69
    endIdx: 70
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 68
    startName: "MLP_18_bwd"
    endName: "MLP_17_bwd"
  }
  connections {
    startIdx: 70
    endIdx: 71
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 69
    startName: "MLP_17_bwd"
    endName: "MLP_16_bwd"
  }
  connections {
    startIdx: 71
    endIdx: 72
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 70
    startName: "MLP_16_bwd"
    endName: "MLP_15_bwd"
  }
  connections {
    startIdx: 72
    endIdx: 73
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 71
    startName: "MLP_15_bwd"
    endName: "MLP_14_bwd"
  }
  connections {
    startIdx: 73
    endIdx: 74
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 72
    startName: "MLP_14_bwd"
    endName: "MLP_13_bwd"
  }
  connections {
    startIdx: 74
    endIdx: 75
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 73
    startName: "MLP_13_bwd"
    endName: "MLP_12_bwd"
  }
  connections {
    startIdx: 75
    endIdx: 76
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 74
    startName: "MLP_12_bwd"
    endName: "MLP_11_bwd"
  }
  connections {
    startIdx: 76
    endIdx: 77
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 75
    startName: "MLP_11_bwd"
    endName: "MLP_10_bwd"
  }
  connections {
    startIdx: 77
    endIdx: 78
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 76
    startName: "MLP_10_bwd"
    endName: "MLP_9_bwd"
  }
  connections {
    startIdx: 78
    endIdx: 79
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 77
    startName: "MLP_9_bwd"
    endName: "MLP_8_bwd"
  }
  connections {
    startIdx: 79
    endIdx: 80
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 78
    startName: "MLP_8_bwd"
    endName: "MLP_7_bwd"
  }
  connections {
    startIdx: 80
    endIdx: 81
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 79
    startName: "MLP_7_bwd"
    endName: "MLP_6_bwd"
  }
  connections {
    startIdx: 81
    endIdx: 82
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 80
    startName: "MLP_6_bwd"
    endName: "MLP_5_bwd"
  }
  connections {
    startIdx: 82
    endIdx: 83
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 81
    startName: "MLP_5_bwd"
    endName: "MLP_4_bwd"
  }
  connections {
    startIdx: 83
    endIdx: 84
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 82
    startName: "MLP_4_bwd"
    endName: "MLP_3_bwd"
  }
  connections {
    startIdx: 84
    endIdx: 85
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 83
    startName: "MLP_3_bwd"
    endName: "MLP_2_bwd"
  }
  connections {
    startIdx: 85
    endIdx: 86
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 84
    startName: "MLP_2_bwd"
    endName: "MLP_1_bwd"
  }
  connections {
    startIdx: 1
    endIdx: 128
    buffer_depth: 42
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 85
    startName: "MLP_1"
    endName: "MLP_2_bwd_weight_update"
  }
  connections {
    startIdx: 2
    endIdx: 127
    buffer_depth: 40
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 86
    startName: "MLP_2"
    endName: "MLP_3_bwd_weight_update"
  }
  connections {
    startIdx: 3
    endIdx: 126
    buffer_depth: 38
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 87
    startName: "MLP_3"
    endName: "MLP_4_bwd_weight_update"
  }
  connections {
    startIdx: 4
    endIdx: 125
    buffer_depth: 36
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 88
    startName: "MLP_4"
    endName: "MLP_5_bwd_weight_update"
  }
  connections {
    startIdx: 5
    endIdx: 124
    buffer_depth: 34
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 89
    startName: "MLP_5"
    endName: "MLP_6_bwd_weight_update"
  }
  connections {
    startIdx: 6
    endIdx: 123
    buffer_depth: 32
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 90
    startName: "MLP_6"
    endName: "MLP_7_bwd_weight_update"
  }
  connections {
    startIdx: 7
    endIdx: 122
    buffer_depth: 30
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 91
    startName: "MLP_7"
    endName: "MLP_8_bwd_weight_update"
  }
  connections {
    startIdx: 8
    endIdx: 121
    buffer_depth: 28
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 92
    startName: "MLP_8"
    endName: "MLP_9_bwd_weight_update"
  }
  connections {
    startIdx: 9
    endIdx: 120
    buffer_depth: 26
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 93
    startName: "MLP_9"
    endName: "MLP_10_bwd_weight_update"
  }
  connections {
    startIdx: 10
    endIdx: 119
    buffer_depth: 24
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 94
    startName: "MLP_10"
    endName: "MLP_11_bwd_weight_update"
  }
  connections {
    startIdx: 11
    endIdx: 118
    buffer_depth: 22
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 95
    startName: "MLP_11"
    endName: "MLP_12_bwd_weight_update"
  }
  connections {
    startIdx: 12
    endIdx: 117
    buffer_depth: 20
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 96
    startName: "MLP_12"
    endName: "MLP_13_bwd_weight_update"
  }
  connections {
    startIdx: 13
    endIdx: 116
    buffer_depth: 18
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 97
    startName: "MLP_13"
    endName: "MLP_14_bwd_weight_update"
  }
  connections {
    startIdx: 14
    endIdx: 115
    buffer_depth: 16
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 98
    startName: "MLP_14"
    endName: "MLP_15_bwd_weight_update"
  }
  connections {
    startIdx: 15
    endIdx: 114
    buffer_depth: 14
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 99
    startName: "MLP_15"
    endName: "MLP_16_bwd_weight_update"
  }
  connections {
    startIdx: 16
    endIdx: 113
    buffer_depth: 12
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 100
    startName: "MLP_16"
    endName: "MLP_17_bwd_weight_update"
  }
  connections {
    startIdx: 17
    endIdx: 112
    buffer_depth: 10
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 101
    startName: "MLP_17"
    endName: "MLP_18_bwd_weight_update"
  }
  connections {
    startIdx: 18
    endIdx: 111
    buffer_depth: 8
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 102
    startName: "MLP_18"
    endName: "MLP_19_bwd_weight_update"
  }
  connections {
    startIdx: 19
    endIdx: 110
    buffer_depth: 6
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 103
    startName: "MLP_19"
    endName: "MLP_20_bwd_weight_update"
  }
  connections {
    startIdx: 20
    endIdx: 109
    buffer_depth: 4
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 104
    startName: "MLP_20"
    endName: "MLP_21_bwd_weight_update"
  }
  connections {
    startIdx: 21
    endIdx: 108
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 105
    startName: "MLP_21"
    endName: "MLP_22_bwd_weight_update"
  }
  connections {
    startIdx: 22
    endIdx: 107
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 106
    startName: "MLP_22"
    endName: "MLP_23_bwd_weight_update"
  }
  connections {
    startIdx: 23
    endIdx: 106
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 107
    startName: "MLP_23"
    endName: "MLP_24_bwd_weight_update"
  }
  connections {
    startIdx: 24
    endIdx: 105
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 108
    startName: "MLP_24"
    endName: "MLP_25_bwd_weight_update"
  }
  connections {
    startIdx: 25
    endIdx: 104
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 109
    startName: "MLP_25"
    endName: "MLP_26_bwd_weight_update"
  }
  connections {
    startIdx: 26
    endIdx: 103
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 110
    startName: "MLP_26"
    endName: "MLP_27_bwd_weight_update"
  }
  connections {
    startIdx: 27
    endIdx: 102
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 111
    startName: "MLP_27"
    endName: "MLP_28_bwd_weight_update"
  }
  connections {
    startIdx: 28
    endIdx: 101
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 112
    startName: "MLP_28"
    endName: "MLP_29_bwd_weight_update"
  }
  connections {
    startIdx: 29
    endIdx: 100
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 113
    startName: "MLP_29"
    endName: "MLP_30_bwd_weight_update"
  }
  connections {
    startIdx: 30
    endIdx: 99
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 114
    startName: "MLP_30"
    endName: "MLP_31_bwd_weight_update"
  }
  connections {
    startIdx: 31
    endIdx: 98
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 115
    startName: "MLP_31"
    endName: "MLP_32_bwd_weight_update"
  }
  connections {
    startIdx: 32
    endIdx: 97
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 116
    startName: "MLP_32"
    endName: "MLP_33_bwd_weight_update"
  }
  connections {
    startIdx: 33
    endIdx: 96
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 117
    startName: "MLP_33"
    endName: "MLP_34_bwd_weight_update"
  }
  connections {
    startIdx: 34
    endIdx: 95
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 118
    startName: "MLP_34"
    endName: "MLP_35_bwd_weight_update"
  }
  connections {
    startIdx: 35
    endIdx: 94
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 119
    startName: "MLP_35"
    endName: "MLP_36_bwd_weight_update"
  }
  connections {
    startIdx: 36
    endIdx: 93
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 120
    startName: "MLP_36"
    endName: "MLP_37_bwd_weight_update"
  }
  connections {
    startIdx: 37
    endIdx: 92
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 121
    startName: "MLP_37"
    endName: "MLP_38_bwd_weight_update"
  }
  connections {
    startIdx: 38
    endIdx: 91
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 122
    startName: "MLP_38"
    endName: "MLP_39_bwd_weight_update"
  }
  connections {
    startIdx: 39
    endIdx: 90
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 123
    startName: "MLP_39"
    endName: "MLP_40_bwd_weight_update"
  }
  connections {
    startIdx: 40
    endIdx: 89
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 124
    startName: "MLP_40"
    endName: "MLP_41_bwd_weight_update"
  }
  connections {
    startIdx: 41
    endIdx: 88
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 125
    startName: "MLP_41"
    endName: "MLP_42_bwd_weight_update"
  }
  connections {
    startIdx: 42
    endIdx: 87
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 126
    startName: "MLP_42"
    endName: "MLP_43_bwd_weight_update"
  }
  connections {
    startIdx: 44
    endIdx: 88
    buffer_depth: 42
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 127
    startName: "MLP_43_bwd"
    endName: "MLP_42_bwd_weight_update"
  }
  connections {
    startIdx: 45
    endIdx: 89
    buffer_depth: 40
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 128
    startName: "MLP_42_bwd"
    endName: "MLP_41_bwd_weight_update"
  }
  connections {
    startIdx: 46
    endIdx: 90
    buffer_depth: 38
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 129
    startName: "MLP_41_bwd"
    endName: "MLP_40_bwd_weight_update"
  }
  connections {
    startIdx: 47
    endIdx: 91
    buffer_depth: 36
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 130
    startName: "MLP_40_bwd"
    endName: "MLP_39_bwd_weight_update"
  }
  connections {
    startIdx: 48
    endIdx: 92
    buffer_depth: 34
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 131
    startName: "MLP_39_bwd"
    endName: "MLP_38_bwd_weight_update"
  }
  connections {
    startIdx: 49
    endIdx: 93
    buffer_depth: 32
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 132
    startName: "MLP_38_bwd"
    endName: "MLP_37_bwd_weight_update"
  }
  connections {
    startIdx: 50
    endIdx: 94
    buffer_depth: 30
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 133
    startName: "MLP_37_bwd"
    endName: "MLP_36_bwd_weight_update"
  }
  connections {
    startIdx: 51
    endIdx: 95
    buffer_depth: 28
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 134
    startName: "MLP_36_bwd"
    endName: "MLP_35_bwd_weight_update"
  }
  connections {
    startIdx: 52
    endIdx: 96
    buffer_depth: 26
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 135
    startName: "MLP_35_bwd"
    endName: "MLP_34_bwd_weight_update"
  }
  connections {
    startIdx: 53
    endIdx: 97
    buffer_depth: 24
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 136
    startName: "MLP_34_bwd"
    endName: "MLP_33_bwd_weight_update"
  }
  connections {
    startIdx: 54
    endIdx: 98
    buffer_depth: 22
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 137
    startName: "MLP_33_bwd"
    endName: "MLP_32_bwd_weight_update"
  }
  connections {
    startIdx: 55
    endIdx: 99
    buffer_depth: 20
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 138
    startName: "MLP_32_bwd"
    endName: "MLP_31_bwd_weight_update"
  }
  connections {
    startIdx: 56
    endIdx: 100
    buffer_depth: 18
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 139
    startName: "MLP_31_bwd"
    endName: "MLP_30_bwd_weight_update"
  }
  connections {
    startIdx: 57
    endIdx: 101
    buffer_depth: 16
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 140
    startName: "MLP_30_bwd"
    endName: "MLP_29_bwd_weight_update"
  }
  connections {
    startIdx: 58
    endIdx: 102
    buffer_depth: 14
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 141
    startName: "MLP_29_bwd"
    endName: "MLP_28_bwd_weight_update"
  }
  connections {
    startIdx: 59
    endIdx: 103
    buffer_depth: 12
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 142
    startName: "MLP_28_bwd"
    endName: "MLP_27_bwd_weight_update"
  }
  connections {
    startIdx: 60
    endIdx: 104
    buffer_depth: 10
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 143
    startName: "MLP_27_bwd"
    endName: "MLP_26_bwd_weight_update"
  }
  connections {
    startIdx: 61
    endIdx: 105
    buffer_depth: 8
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 144
    startName: "MLP_26_bwd"
    endName: "MLP_25_bwd_weight_update"
  }
  connections {
    startIdx: 62
    endIdx: 106
    buffer_depth: 6
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 145
    startName: "MLP_25_bwd"
    endName: "MLP_24_bwd_weight_update"
  }
  connections {
    startIdx: 63
    endIdx: 107
    buffer_depth: 4
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 146
    startName: "MLP_24_bwd"
    endName: "MLP_23_bwd_weight_update"
  }
  connections {
    startIdx: 64
    endIdx: 108
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 147
    startName: "MLP_23_bwd"
    endName: "MLP_22_bwd_weight_update"
  }
  connections {
    startIdx: 65
    endIdx: 109
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 148
    startName: "MLP_22_bwd"
    endName: "MLP_21_bwd_weight_update"
  }
  connections {
    startIdx: 66
    endIdx: 110
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 149
    startName: "MLP_21_bwd"
    endName: "MLP_20_bwd_weight_update"
  }
  connections {
    startIdx: 67
    endIdx: 111
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 150
    startName: "MLP_20_bwd"
    endName: "MLP_19_bwd_weight_update"
  }
  connections {
    startIdx: 68
    endIdx: 112
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 151
    startName: "MLP_19_bwd"
    endName: "MLP_18_bwd_weight_update"
  }
  connections {
    startIdx: 69
    endIdx: 113
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 152
    startName: "MLP_18_bwd"
    endName: "MLP_17_bwd_weight_update"
  }
  connections {
    startIdx: 70
    endIdx: 114
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 153
    startName: "MLP_17_bwd"
    endName: "MLP_16_bwd_weight_update"
  }
  connections {
    startIdx: 71
    endIdx: 115
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 154
    startName: "MLP_16_bwd"
    endName: "MLP_15_bwd_weight_update"
  }
  connections {
    startIdx: 72
    endIdx: 116
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 155
    startName: "MLP_15_bwd"
    endName: "MLP_14_bwd_weight_update"
  }
  connections {
    startIdx: 73
    endIdx: 117
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 156
    startName: "MLP_14_bwd"
    endName: "MLP_13_bwd_weight_update"
  }
  connections {
    startIdx: 74
    endIdx: 118
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 157
    startName: "MLP_13_bwd"
    endName: "MLP_12_bwd_weight_update"
  }
  connections {
    startIdx: 75
    endIdx: 119
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 158
    startName: "MLP_12_bwd"
    endName: "MLP_11_bwd_weight_update"
  }
  connections {
    startIdx: 76
    endIdx: 120
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 159
    startName: "MLP_11_bwd"
    endName: "MLP_10_bwd_weight_update"
  }
  connections {
    startIdx: 77
    endIdx: 121
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 160
    startName: "MLP_10_bwd"
    endName: "MLP_9_bwd_weight_update"
  }
  connections {
    startIdx: 78
    endIdx: 122
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 161
    startName: "MLP_9_bwd"
    endName: "MLP_8_bwd_weight_update"
  }
  connections {
    startIdx: 79
    endIdx: 123
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 162
    startName: "MLP_8_bwd"
    endName: "MLP_7_bwd_weight_update"
  }
  connections {
    startIdx: 80
    endIdx: 124
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 163
    startName: "MLP_7_bwd"
    endName: "MLP_6_bwd_weight_update"
  }
  connections {
    startIdx: 81
    endIdx: 125
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 164
    startName: "MLP_6_bwd"
    endName: "MLP_5_bwd_weight_update"
  }
  connections {
    startIdx: 82
    endIdx: 126
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 165
    startName: "MLP_5_bwd"
    endName: "MLP_4_bwd_weight_update"
  }
  connections {
    startIdx: 83
    endIdx: 127
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 166
    startName: "MLP_4_bwd"
    endName: "MLP_3_bwd_weight_update"
  }
  connections {
    startIdx: 84
    endIdx: 128
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 167
    startName: "MLP_3_bwd"
    endName: "MLP_2_bwd_weight_update"
  }
  connections {
    startIdx: 85
    endIdx: 129
    buffer_depth: 2
    tensor_size: 2793472.0
    shard_tensor_size: 2793472.0
    id: 168
    startName: "MLP_2_bwd"
    endName: "MLP_1_bwd_weight_update"
  }
}
system {
  num_chip: 128
  accelerator {
    core: 432
    systolic_width: 16
    systolic_height: 16
    sram_cap: 88080380.0
    freq: 1.41
  }
  r_fc {
    x: 8
    y: 16
    link_bw_x: 600.0
    link_bw_y: 25.0
  }
  memory {
    dram_bw: 1555.0
    dram_cap: 42949673000.0
  }
}
cost {
  link_unit_price: 2.0
  switch_unit_price: 24.0
  dram_unit_price: 1.0
  accelerator_price: 20000.0
  link_unit_power_x: 0.0104
  link_unit_power_y: 0.052
  dram_unit_power: 0.16248
  accelerator_power: 511.57712
}
miscellany {
  dlrm {
    num_table: 50
    emb_dim: 92
    row: 72173913
    global_batch_size: 128000000
  }
  execution_style: KERNEL_BY_KERNEL
  compute_util: 0.9
  word: 2
}
gurobi {
  thread: 144
  gap: 0.001
  time: 3600
}
