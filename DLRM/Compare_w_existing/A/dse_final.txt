dataflow_graph {
  kernels {
    name: "MLP_1"
    id: 1
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 3375
      K: 3375
      N: 512
      input_tensor_size: 3456000.0
      weight_tensor_size: 22781250.0
      output_tensor_size: 3456000.0
      sharding: NO_SHARDING
      shard_outer_M: 3375
      shard_K: 3375
      shard_N: 512
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_2"
    id: 2
    topological_number: 1
    config: 1
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 3375
      K: 3375
      N: 512
      input_tensor_size: 3456000.0
      weight_tensor_size: 22781250.0
      output_tensor_size: 3456000.0
      sharding: NO_SHARDING
      shard_outer_M: 3375
      shard_K: 3375
      shard_N: 512
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_3"
    id: 3
    topological_number: 2
    config: 2
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 3375
      K: 3375
      N: 512
      input_tensor_size: 3456000.0
      weight_tensor_size: 22781250.0
      output_tensor_size: 3456000.0
      sharding: NO_SHARDING
      shard_outer_M: 3375
      shard_K: 3375
      shard_N: 512
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_4"
    id: 4
    topological_number: 3
    config: 3
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 3375
      K: 3375
      N: 512
      input_tensor_size: 3456000.0
      weight_tensor_size: 22781250.0
      output_tensor_size: 3456000.0
      sharding: NO_SHARDING
      shard_outer_M: 3375
      shard_K: 3375
      shard_N: 512
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_5"
    id: 5
    topological_number: 4
    config: 4
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 3375
      K: 3375
      N: 512
      input_tensor_size: 3456000.0
      weight_tensor_size: 22781250.0
      output_tensor_size: 3456000.0
      sharding: NO_SHARDING
      shard_outer_M: 3375
      shard_K: 3375
      shard_N: 512
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_6"
    id: 6
    topological_number: 5
    config: 5
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 3375
      K: 3375
      N: 512
      input_tensor_size: 3456000.0
      weight_tensor_size: 22781250.0
      output_tensor_size: 3456000.0
      sharding: NO_SHARDING
      shard_outer_M: 3375
      shard_K: 3375
      shard_N: 512
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_7"
    id: 7
    topological_number: 6
    config: 6
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 3375
      K: 3375
      N: 512
      input_tensor_size: 3456000.0
      weight_tensor_size: 22781250.0
      output_tensor_size: 3456000.0
      sharding: NO_SHARDING
      shard_outer_M: 3375
      shard_K: 3375
      shard_N: 512
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_8"
    id: 8
    topological_number: 7
    config: 7
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 3375
      K: 3375
      N: 512
      input_tensor_size: 3456000.0
      weight_tensor_size: 22781250.0
      output_tensor_size: 3456000.0
      sharding: NO_SHARDING
      shard_outer_M: 3375
      shard_K: 3375
      shard_N: 512
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_9"
    id: 9
    topological_number: 8
    config: 8
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 3375
      K: 3375
      N: 512
      input_tensor_size: 3456000.0
      weight_tensor_size: 22781250.0
      output_tensor_size: 3456000.0
      sharding: NO_SHARDING
      shard_outer_M: 3375
      shard_K: 3375
      shard_N: 512
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_10"
    id: 10
    topological_number: 9
    config: 9
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 3375
      K: 3375
      N: 512
      input_tensor_size: 3456000.0
      weight_tensor_size: 22781250.0
      output_tensor_size: 3456000.0
      sharding: NO_SHARDING
      communication_type: ALL_TO_ALL
      communication_size: 1142784000.0
      shard_outer_M: 3375
      shard_K: 3375
      shard_N: 512
      tiling: NO_TILING
      memory_size: 1142784000.0
    }
  }
  kernels {
    name: "MLP_11"
    id: 11
    topological_number: 10
    config: 10
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 3375
      K: 3375
      N: 512
      input_tensor_size: 3456000.0
      weight_tensor_size: 22781250.0
      output_tensor_size: 3456000.0
      sharding: NO_SHARDING
      shard_outer_M: 3375
      shard_K: 3375
      shard_N: 512
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_12"
    id: 12
    topological_number: 11
    config: 11
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 3375
      K: 3375
      N: 512
      input_tensor_size: 3456000.0
      weight_tensor_size: 22781250.0
      output_tensor_size: 3456000.0
      sharding: NO_SHARDING
      shard_outer_M: 3375
      shard_K: 3375
      shard_N: 512
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_13"
    id: 13
    topological_number: 12
    config: 12
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 3375
      K: 3375
      N: 512
      input_tensor_size: 3456000.0
      weight_tensor_size: 22781250.0
      output_tensor_size: 3456000.0
      sharding: NO_SHARDING
      shard_outer_M: 3375
      shard_K: 3375
      shard_N: 512
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_14"
    id: 14
    topological_number: 13
    config: 13
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 3375
      K: 3375
      N: 512
      input_tensor_size: 3456000.0
      weight_tensor_size: 22781250.0
      output_tensor_size: 3456000.0
      sharding: NO_SHARDING
      shard_outer_M: 3375
      shard_K: 3375
      shard_N: 512
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_15"
    id: 15
    topological_number: 14
    config: 14
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 3375
      K: 3375
      N: 512
      input_tensor_size: 3456000.0
      weight_tensor_size: 22781250.0
      output_tensor_size: 3456000.0
      sharding: NO_SHARDING
      shard_outer_M: 3375
      shard_K: 3375
      shard_N: 512
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_16"
    id: 16
    topological_number: 15
    config: 15
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 3375
      K: 3375
      N: 512
      input_tensor_size: 3456000.0
      weight_tensor_size: 22781250.0
      output_tensor_size: 3456000.0
      sharding: NO_SHARDING
      shard_outer_M: 3375
      shard_K: 3375
      shard_N: 512
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_17"
    id: 17
    topological_number: 16
    config: 16
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 3375
      K: 3375
      N: 512
      input_tensor_size: 3456000.0
      weight_tensor_size: 22781250.0
      output_tensor_size: 3456000.0
      sharding: NO_SHARDING
      shard_outer_M: 3375
      shard_K: 3375
      shard_N: 512
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_18"
    id: 18
    topological_number: 17
    config: 17
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 3375
      K: 3375
      N: 512
      input_tensor_size: 3456000.0
      weight_tensor_size: 22781250.0
      output_tensor_size: 3456000.0
      sharding: NO_SHARDING
      shard_outer_M: 3375
      shard_K: 3375
      shard_N: 512
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_19"
    id: 19
    topological_number: 18
    config: 18
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 3375
      K: 3375
      N: 512
      input_tensor_size: 3456000.0
      weight_tensor_size: 22781250.0
      output_tensor_size: 3456000.0
      sharding: NO_SHARDING
      shard_outer_M: 3375
      shard_K: 3375
      shard_N: 512
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_20"
    id: 20
    topological_number: 19
    config: 19
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 3375
      K: 3375
      N: 512
      input_tensor_size: 3456000.0
      weight_tensor_size: 22781250.0
      output_tensor_size: 3456000.0
      sharding: NO_SHARDING
      shard_outer_M: 3375
      shard_K: 3375
      shard_N: 512
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_20_bwd"
    id: 21
    config: 20
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 3375
      K: 3375
      N: 512
      input_tensor_size: 3456000.0
      weight_tensor_size: 22781250.0
      output_tensor_size: 3456000.0
      sharding: NO_SHARDING
      shard_outer_M: 3375
      shard_K: 3375
      shard_N: 512
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_19_bwd"
    id: 22
    topological_number: 1
    config: 21
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 3375
      K: 3375
      N: 512
      input_tensor_size: 3456000.0
      weight_tensor_size: 22781250.0
      output_tensor_size: 3456000.0
      sharding: NO_SHARDING
      shard_outer_M: 3375
      shard_K: 3375
      shard_N: 512
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_18_bwd"
    id: 23
    topological_number: 2
    config: 22
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 3375
      K: 3375
      N: 512
      input_tensor_size: 3456000.0
      weight_tensor_size: 22781250.0
      output_tensor_size: 3456000.0
      sharding: NO_SHARDING
      shard_outer_M: 3375
      shard_K: 3375
      shard_N: 512
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_17_bwd"
    id: 24
    topological_number: 3
    config: 23
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 3375
      K: 3375
      N: 512
      input_tensor_size: 3456000.0
      weight_tensor_size: 22781250.0
      output_tensor_size: 3456000.0
      sharding: NO_SHARDING
      shard_outer_M: 3375
      shard_K: 3375
      shard_N: 512
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_16_bwd"
    id: 25
    topological_number: 4
    config: 24
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 3375
      K: 3375
      N: 512
      input_tensor_size: 3456000.0
      weight_tensor_size: 22781250.0
      output_tensor_size: 3456000.0
      sharding: NO_SHARDING
      shard_outer_M: 3375
      shard_K: 3375
      shard_N: 512
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_15_bwd"
    id: 26
    topological_number: 5
    config: 25
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 3375
      K: 3375
      N: 512
      input_tensor_size: 3456000.0
      weight_tensor_size: 22781250.0
      output_tensor_size: 3456000.0
      sharding: NO_SHARDING
      shard_outer_M: 3375
      shard_K: 3375
      shard_N: 512
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_14_bwd"
    id: 27
    topological_number: 6
    config: 26
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 3375
      K: 3375
      N: 512
      input_tensor_size: 3456000.0
      weight_tensor_size: 22781250.0
      output_tensor_size: 3456000.0
      sharding: NO_SHARDING
      shard_outer_M: 3375
      shard_K: 3375
      shard_N: 512
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_13_bwd"
    id: 28
    topological_number: 7
    config: 27
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 3375
      K: 3375
      N: 512
      input_tensor_size: 3456000.0
      weight_tensor_size: 22781250.0
      output_tensor_size: 3456000.0
      sharding: NO_SHARDING
      shard_outer_M: 3375
      shard_K: 3375
      shard_N: 512
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_12_bwd"
    id: 29
    topological_number: 8
    config: 28
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 3375
      K: 3375
      N: 512
      input_tensor_size: 3456000.0
      weight_tensor_size: 22781250.0
      output_tensor_size: 3456000.0
      sharding: NO_SHARDING
      shard_outer_M: 3375
      shard_K: 3375
      shard_N: 512
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_11_bwd"
    id: 30
    topological_number: 9
    config: 29
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 3375
      K: 3375
      N: 512
      input_tensor_size: 3456000.0
      weight_tensor_size: 22781250.0
      output_tensor_size: 3456000.0
      sharding: NO_SHARDING
      shard_outer_M: 3375
      shard_K: 3375
      shard_N: 512
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_10_bwd"
    id: 31
    topological_number: 10
    config: 30
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 3375
      K: 3375
      N: 512
      input_tensor_size: 3456000.0
      weight_tensor_size: 22781250.0
      output_tensor_size: 3456000.0
      sharding: NO_SHARDING
      communication_type: ALL_TO_ALL
      communication_size: 1142784000.0
      shard_outer_M: 3375
      shard_K: 3375
      shard_N: 512
      tiling: NO_TILING
      memory_size: 1142784000.0
    }
  }
  kernels {
    name: "MLP_9_bwd"
    id: 32
    topological_number: 11
    config: 31
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 3375
      K: 3375
      N: 512
      input_tensor_size: 3456000.0
      weight_tensor_size: 22781250.0
      output_tensor_size: 3456000.0
      sharding: NO_SHARDING
      shard_outer_M: 3375
      shard_K: 3375
      shard_N: 512
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_8_bwd"
    id: 33
    topological_number: 12
    config: 32
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 3375
      K: 3375
      N: 512
      input_tensor_size: 3456000.0
      weight_tensor_size: 22781250.0
      output_tensor_size: 3456000.0
      sharding: NO_SHARDING
      shard_outer_M: 3375
      shard_K: 3375
      shard_N: 512
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_7_bwd"
    id: 34
    topological_number: 13
    config: 33
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 3375
      K: 3375
      N: 512
      input_tensor_size: 3456000.0
      weight_tensor_size: 22781250.0
      output_tensor_size: 3456000.0
      sharding: NO_SHARDING
      shard_outer_M: 3375
      shard_K: 3375
      shard_N: 512
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_6_bwd"
    id: 35
    topological_number: 14
    config: 34
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 3375
      K: 3375
      N: 512
      input_tensor_size: 3456000.0
      weight_tensor_size: 22781250.0
      output_tensor_size: 3456000.0
      sharding: NO_SHARDING
      shard_outer_M: 3375
      shard_K: 3375
      shard_N: 512
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_5_bwd"
    id: 36
    topological_number: 15
    config: 35
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 3375
      K: 3375
      N: 512
      input_tensor_size: 3456000.0
      weight_tensor_size: 22781250.0
      output_tensor_size: 3456000.0
      sharding: NO_SHARDING
      shard_outer_M: 3375
      shard_K: 3375
      shard_N: 512
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_4_bwd"
    id: 37
    topological_number: 16
    config: 36
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 3375
      K: 3375
      N: 512
      input_tensor_size: 3456000.0
      weight_tensor_size: 22781250.0
      output_tensor_size: 3456000.0
      sharding: NO_SHARDING
      shard_outer_M: 3375
      shard_K: 3375
      shard_N: 512
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_3_bwd"
    id: 38
    topological_number: 17
    config: 37
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 3375
      K: 3375
      N: 512
      input_tensor_size: 3456000.0
      weight_tensor_size: 22781250.0
      output_tensor_size: 3456000.0
      sharding: NO_SHARDING
      shard_outer_M: 3375
      shard_K: 3375
      shard_N: 512
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_2_bwd"
    id: 39
    topological_number: 18
    config: 38
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 3375
      K: 3375
      N: 512
      input_tensor_size: 3456000.0
      weight_tensor_size: 22781250.0
      output_tensor_size: 3456000.0
      sharding: NO_SHARDING
      shard_outer_M: 3375
      shard_K: 3375
      shard_N: 512
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_1_bwd"
    id: 40
    topological_number: 19
    config: 39
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 3375
      K: 3375
      N: 512
      input_tensor_size: 3456000.0
      weight_tensor_size: 22781250.0
      output_tensor_size: 3456000.0
      sharding: NO_SHARDING
      shard_outer_M: 3375
      shard_K: 3375
      shard_N: 512
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_20_bwd_weight_update"
    id: 41
    topological_number: 19
    config: 40
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 3375
      K: 512
      N: 3375
      input_tensor_1_size: 3456000.0
      input_tensor_2_size: 3456000.0
      output_tensor_size: 22781250.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 22781250.0
      shard_outer_M: 3375
      shard_K: 512
      shard_N: 3375
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_19_bwd_weight_update"
    id: 42
    topological_number: 18
    config: 41
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 3375
      K: 512
      N: 3375
      input_tensor_1_size: 3456000.0
      input_tensor_2_size: 3456000.0
      output_tensor_size: 22781250.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 22781250.0
      shard_outer_M: 3375
      shard_K: 512
      shard_N: 3375
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_18_bwd_weight_update"
    id: 43
    topological_number: 17
    config: 42
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 3375
      K: 512
      N: 3375
      input_tensor_1_size: 3456000.0
      input_tensor_2_size: 3456000.0
      output_tensor_size: 22781250.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 22781250.0
      shard_outer_M: 3375
      shard_K: 512
      shard_N: 3375
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_17_bwd_weight_update"
    id: 44
    topological_number: 16
    config: 43
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 3375
      K: 512
      N: 3375
      input_tensor_1_size: 3456000.0
      input_tensor_2_size: 3456000.0
      output_tensor_size: 22781250.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 22781250.0
      shard_outer_M: 3375
      shard_K: 512
      shard_N: 3375
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_16_bwd_weight_update"
    id: 45
    topological_number: 15
    config: 44
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 3375
      K: 512
      N: 3375
      input_tensor_1_size: 3456000.0
      input_tensor_2_size: 3456000.0
      output_tensor_size: 22781250.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 22781250.0
      shard_outer_M: 3375
      shard_K: 512
      shard_N: 3375
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_15_bwd_weight_update"
    id: 46
    topological_number: 14
    config: 45
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 3375
      K: 512
      N: 3375
      input_tensor_1_size: 3456000.0
      input_tensor_2_size: 3456000.0
      output_tensor_size: 22781250.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 22781250.0
      shard_outer_M: 3375
      shard_K: 512
      shard_N: 3375
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_14_bwd_weight_update"
    id: 47
    topological_number: 13
    config: 46
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 3375
      K: 512
      N: 3375
      input_tensor_1_size: 3456000.0
      input_tensor_2_size: 3456000.0
      output_tensor_size: 22781250.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 22781250.0
      shard_outer_M: 3375
      shard_K: 512
      shard_N: 3375
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_13_bwd_weight_update"
    id: 48
    topological_number: 12
    config: 47
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 3375
      K: 512
      N: 3375
      input_tensor_1_size: 3456000.0
      input_tensor_2_size: 3456000.0
      output_tensor_size: 22781250.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 22781250.0
      shard_outer_M: 3375
      shard_K: 512
      shard_N: 3375
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_12_bwd_weight_update"
    id: 49
    topological_number: 11
    config: 48
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 3375
      K: 512
      N: 3375
      input_tensor_1_size: 3456000.0
      input_tensor_2_size: 3456000.0
      output_tensor_size: 22781250.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 22781250.0
      shard_outer_M: 3375
      shard_K: 512
      shard_N: 3375
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_11_bwd_weight_update"
    id: 50
    topological_number: 10
    config: 49
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 3375
      K: 512
      N: 3375
      input_tensor_1_size: 3456000.0
      input_tensor_2_size: 3456000.0
      output_tensor_size: 22781250.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 22781250.0
      shard_outer_M: 3375
      shard_K: 512
      shard_N: 3375
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_10_bwd_weight_update"
    id: 51
    topological_number: 10
    config: 50
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 3375
      K: 512
      N: 3375
      input_tensor_1_size: 3456000.0
      input_tensor_2_size: 3456000.0
      output_tensor_size: 22781250.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 22781250.0
      shard_outer_M: 3375
      shard_K: 512
      shard_N: 3375
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_9_bwd_weight_update"
    id: 52
    topological_number: 11
    config: 51
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 3375
      K: 512
      N: 3375
      input_tensor_1_size: 3456000.0
      input_tensor_2_size: 3456000.0
      output_tensor_size: 22781250.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 22781250.0
      shard_outer_M: 3375
      shard_K: 512
      shard_N: 3375
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_8_bwd_weight_update"
    id: 53
    topological_number: 12
    config: 52
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 3375
      K: 512
      N: 3375
      input_tensor_1_size: 3456000.0
      input_tensor_2_size: 3456000.0
      output_tensor_size: 22781250.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 22781250.0
      shard_outer_M: 3375
      shard_K: 512
      shard_N: 3375
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_7_bwd_weight_update"
    id: 54
    topological_number: 13
    config: 53
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 3375
      K: 512
      N: 3375
      input_tensor_1_size: 3456000.0
      input_tensor_2_size: 3456000.0
      output_tensor_size: 22781250.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 22781250.0
      shard_outer_M: 3375
      shard_K: 512
      shard_N: 3375
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_6_bwd_weight_update"
    id: 55
    topological_number: 14
    config: 54
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 3375
      K: 512
      N: 3375
      input_tensor_1_size: 3456000.0
      input_tensor_2_size: 3456000.0
      output_tensor_size: 22781250.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 22781250.0
      shard_outer_M: 3375
      shard_K: 512
      shard_N: 3375
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_5_bwd_weight_update"
    id: 56
    topological_number: 15
    config: 55
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 3375
      K: 512
      N: 3375
      input_tensor_1_size: 3456000.0
      input_tensor_2_size: 3456000.0
      output_tensor_size: 22781250.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 22781250.0
      shard_outer_M: 3375
      shard_K: 512
      shard_N: 3375
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_4_bwd_weight_update"
    id: 57
    topological_number: 16
    config: 56
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 3375
      K: 512
      N: 3375
      input_tensor_1_size: 3456000.0
      input_tensor_2_size: 3456000.0
      output_tensor_size: 22781250.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 22781250.0
      shard_outer_M: 3375
      shard_K: 512
      shard_N: 3375
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_3_bwd_weight_update"
    id: 58
    topological_number: 17
    config: 57
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 3375
      K: 512
      N: 3375
      input_tensor_1_size: 3456000.0
      input_tensor_2_size: 3456000.0
      output_tensor_size: 22781250.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 22781250.0
      shard_outer_M: 3375
      shard_K: 512
      shard_N: 3375
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_2_bwd_weight_update"
    id: 59
    topological_number: 18
    config: 58
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 3375
      K: 512
      N: 3375
      input_tensor_1_size: 3456000.0
      input_tensor_2_size: 3456000.0
      output_tensor_size: 22781250.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 22781250.0
      shard_outer_M: 3375
      shard_K: 512
      shard_N: 3375
      tiling: NO_TILING
    }
  }
  kernels {
    name: "MLP_1_bwd_weight_update"
    id: 60
    topological_number: 19
    config: 59
    fwd_bwd: BWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 1
      M: 3375
      K: 512
      N: 3375
      input_tensor_1_size: 3456000.0
      input_tensor_2_size: 3456000.0
      output_tensor_size: 22781250.0
      sharding: NO_SHARDING
      communication_type: ALL_REDUCE_PERIODIC
      communication_size: 22781250.0
      shard_outer_M: 3375
      shard_K: 512
      shard_N: 3375
      tiling: NO_TILING
    }
  }
  connections {
    startIdx: 1
    endIdx: 2
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 1
    startName: "MLP_1"
    endName: "MLP_2"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 2
    endIdx: 3
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 2
    startName: "MLP_2"
    endName: "MLP_3"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 3
    endIdx: 4
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 3
    startName: "MLP_3"
    endName: "MLP_4"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 4
    endIdx: 5
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 4
    startName: "MLP_4"
    endName: "MLP_5"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 5
    endIdx: 6
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 5
    startName: "MLP_5"
    endName: "MLP_6"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 6
    endIdx: 7
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 6
    startName: "MLP_6"
    endName: "MLP_7"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 7
    endIdx: 8
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 7
    startName: "MLP_7"
    endName: "MLP_8"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 8
    endIdx: 9
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 8
    startName: "MLP_8"
    endName: "MLP_9"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 9
    endIdx: 10
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 9
    startName: "MLP_9"
    endName: "MLP_10"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 10
    endIdx: 11
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 10
    startName: "MLP_10"
    endName: "MLP_11"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 11
    endIdx: 12
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 11
    startName: "MLP_11"
    endName: "MLP_12"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 12
    endIdx: 13
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 12
    startName: "MLP_12"
    endName: "MLP_13"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 13
    endIdx: 14
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 13
    startName: "MLP_13"
    endName: "MLP_14"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 14
    endIdx: 15
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 14
    startName: "MLP_14"
    endName: "MLP_15"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 15
    endIdx: 16
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 15
    startName: "MLP_15"
    endName: "MLP_16"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 16
    endIdx: 17
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 16
    startName: "MLP_16"
    endName: "MLP_17"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 17
    endIdx: 18
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 17
    startName: "MLP_17"
    endName: "MLP_18"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 18
    endIdx: 19
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 18
    startName: "MLP_18"
    endName: "MLP_19"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 19
    endIdx: 20
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 19
    startName: "MLP_19"
    endName: "MLP_20"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 21
    endIdx: 22
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 20
    startName: "MLP_20_bwd"
    endName: "MLP_19_bwd"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 22
    endIdx: 23
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 21
    startName: "MLP_19_bwd"
    endName: "MLP_18_bwd"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 23
    endIdx: 24
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 22
    startName: "MLP_18_bwd"
    endName: "MLP_17_bwd"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 24
    endIdx: 25
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 23
    startName: "MLP_17_bwd"
    endName: "MLP_16_bwd"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 25
    endIdx: 26
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 24
    startName: "MLP_16_bwd"
    endName: "MLP_15_bwd"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 26
    endIdx: 27
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 25
    startName: "MLP_15_bwd"
    endName: "MLP_14_bwd"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 27
    endIdx: 28
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 26
    startName: "MLP_14_bwd"
    endName: "MLP_13_bwd"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 28
    endIdx: 29
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 27
    startName: "MLP_13_bwd"
    endName: "MLP_12_bwd"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 29
    endIdx: 30
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 28
    startName: "MLP_12_bwd"
    endName: "MLP_11_bwd"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 30
    endIdx: 31
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 29
    startName: "MLP_11_bwd"
    endName: "MLP_10_bwd"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 31
    endIdx: 32
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 30
    startName: "MLP_10_bwd"
    endName: "MLP_9_bwd"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 32
    endIdx: 33
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 31
    startName: "MLP_9_bwd"
    endName: "MLP_8_bwd"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 33
    endIdx: 34
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 32
    startName: "MLP_8_bwd"
    endName: "MLP_7_bwd"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 34
    endIdx: 35
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 33
    startName: "MLP_7_bwd"
    endName: "MLP_6_bwd"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 35
    endIdx: 36
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 34
    startName: "MLP_6_bwd"
    endName: "MLP_5_bwd"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 36
    endIdx: 37
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 35
    startName: "MLP_5_bwd"
    endName: "MLP_4_bwd"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 37
    endIdx: 38
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 36
    startName: "MLP_4_bwd"
    endName: "MLP_3_bwd"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 38
    endIdx: 39
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 37
    startName: "MLP_3_bwd"
    endName: "MLP_2_bwd"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 39
    endIdx: 40
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 38
    startName: "MLP_2_bwd"
    endName: "MLP_1_bwd"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 1
    endIdx: 59
    buffer_depth: 19
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 39
    startName: "MLP_1"
    endName: "MLP_2_bwd_weight_update"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 2
    endIdx: 58
    buffer_depth: 17
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 40
    startName: "MLP_2"
    endName: "MLP_3_bwd_weight_update"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 3
    endIdx: 57
    buffer_depth: 15
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 41
    startName: "MLP_3"
    endName: "MLP_4_bwd_weight_update"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 4
    endIdx: 56
    buffer_depth: 13
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 42
    startName: "MLP_4"
    endName: "MLP_5_bwd_weight_update"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 5
    endIdx: 55
    buffer_depth: 11
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 43
    startName: "MLP_5"
    endName: "MLP_6_bwd_weight_update"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 6
    endIdx: 54
    buffer_depth: 9
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 44
    startName: "MLP_6"
    endName: "MLP_7_bwd_weight_update"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 7
    endIdx: 53
    buffer_depth: 7
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 45
    startName: "MLP_7"
    endName: "MLP_8_bwd_weight_update"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 8
    endIdx: 52
    buffer_depth: 5
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 46
    startName: "MLP_8"
    endName: "MLP_9_bwd_weight_update"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 9
    endIdx: 51
    buffer_depth: 3
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 47
    startName: "MLP_9"
    endName: "MLP_10_bwd_weight_update"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 10
    endIdx: 50
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 48
    startName: "MLP_10"
    endName: "MLP_11_bwd_weight_update"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 11
    endIdx: 49
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 49
    startName: "MLP_11"
    endName: "MLP_12_bwd_weight_update"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 12
    endIdx: 48
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 50
    startName: "MLP_12"
    endName: "MLP_13_bwd_weight_update"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 13
    endIdx: 47
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 51
    startName: "MLP_13"
    endName: "MLP_14_bwd_weight_update"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 14
    endIdx: 46
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 52
    startName: "MLP_14"
    endName: "MLP_15_bwd_weight_update"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 15
    endIdx: 45
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 53
    startName: "MLP_15"
    endName: "MLP_16_bwd_weight_update"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 16
    endIdx: 44
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 54
    startName: "MLP_16"
    endName: "MLP_17_bwd_weight_update"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 17
    endIdx: 43
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 55
    startName: "MLP_17"
    endName: "MLP_18_bwd_weight_update"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 18
    endIdx: 42
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 56
    startName: "MLP_18"
    endName: "MLP_19_bwd_weight_update"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 19
    endIdx: 41
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 57
    startName: "MLP_19"
    endName: "MLP_20_bwd_weight_update"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 21
    endIdx: 42
    buffer_depth: 19
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 58
    startName: "MLP_20_bwd"
    endName: "MLP_19_bwd_weight_update"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 22
    endIdx: 43
    buffer_depth: 17
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 59
    startName: "MLP_19_bwd"
    endName: "MLP_18_bwd_weight_update"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 23
    endIdx: 44
    buffer_depth: 15
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 60
    startName: "MLP_18_bwd"
    endName: "MLP_17_bwd_weight_update"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 24
    endIdx: 45
    buffer_depth: 13
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 61
    startName: "MLP_17_bwd"
    endName: "MLP_16_bwd_weight_update"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 25
    endIdx: 46
    buffer_depth: 11
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 62
    startName: "MLP_16_bwd"
    endName: "MLP_15_bwd_weight_update"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 26
    endIdx: 47
    buffer_depth: 9
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 63
    startName: "MLP_15_bwd"
    endName: "MLP_14_bwd_weight_update"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 27
    endIdx: 48
    buffer_depth: 7
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 64
    startName: "MLP_14_bwd"
    endName: "MLP_13_bwd_weight_update"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 28
    endIdx: 49
    buffer_depth: 5
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 65
    startName: "MLP_13_bwd"
    endName: "MLP_12_bwd_weight_update"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 29
    endIdx: 50
    buffer_depth: 3
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 66
    startName: "MLP_12_bwd"
    endName: "MLP_11_bwd_weight_update"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 30
    endIdx: 51
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 67
    startName: "MLP_11_bwd"
    endName: "MLP_10_bwd_weight_update"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 31
    endIdx: 52
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 68
    startName: "MLP_10_bwd"
    endName: "MLP_9_bwd_weight_update"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 32
    endIdx: 53
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 69
    startName: "MLP_9_bwd"
    endName: "MLP_8_bwd_weight_update"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 33
    endIdx: 54
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 70
    startName: "MLP_8_bwd"
    endName: "MLP_7_bwd_weight_update"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 34
    endIdx: 55
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 71
    startName: "MLP_7_bwd"
    endName: "MLP_6_bwd_weight_update"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 35
    endIdx: 56
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 72
    startName: "MLP_6_bwd"
    endName: "MLP_5_bwd_weight_update"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 36
    endIdx: 57
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 73
    startName: "MLP_5_bwd"
    endName: "MLP_4_bwd_weight_update"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 37
    endIdx: 58
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 74
    startName: "MLP_4_bwd"
    endName: "MLP_3_bwd_weight_update"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 38
    endIdx: 59
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 75
    startName: "MLP_3_bwd"
    endName: "MLP_2_bwd_weight_update"
    lane_stage_type: LANE
  }
  connections {
    startIdx: 39
    endIdx: 60
    buffer_depth: 2
    tensor_size: 3456000.0
    shard_tensor_size: 3456000.0
    id: 76
    startName: "MLP_2_bwd"
    endName: "MLP_1_bwd_weight_update"
    lane_stage_type: LANE
  }
}
system {
  num_chip: 128
  accelerator {
    core: 432
    systolic_width: 16
    systolic_height: 16
    sram_cap: 88080380.0
    freq: 1.41
  }
  r_fc {
    x: 8
    y: 16
    link_bw_x: 300.0
    link_bw_y: 25.0
  }
  memory {
    dram_bw: 1555.0
    dram_cap: 42949673000.0
  }
}
cost {
  link_unit_price: 2.0
  switch_unit_price: 24.0
  dram_unit_price: 1.0
  accelerator_price: 20000.0
  link_unit_power_x: 0.0104
  link_unit_power_y: 0.052
  dram_unit_power: 0.16248
  accelerator_power: 511.57712
}
execution {
  dlrm {
    num_table: 800
    emb_dim: 93
    row: 10658602
    global_batch_size: 128000000
    num_copy: 1
  }
  execution_style: KERNEL_BY_KERNEL
  overlap: PERFECT_OVERLAP
  word: 2
}
gurobi {
  gap: 0.001
  time: 180
}
