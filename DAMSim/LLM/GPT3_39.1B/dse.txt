dataflow_graph {
  kernels {
    name: "Add_Prev_Layer"
    id: 1
    fwd_bwd: FWD
    type: SIMD
    elementwise_input1 {
      outer: 1
      M: 8192
      N: 2048
      input_tensor_size: 33554432.0
      output_tensor_size: 33554432.0
      tiling: N_TILING
    }
  }
  kernels {
    name: "LayerNorm_1"
    id: 2
    fwd_bwd: FWD
    type: SIMD
    elementwise_input1 {
      outer: 1
      M: 8192
      N: 2048
      input_tensor_size: 33554432.0
      output_tensor_size: 33554432.0
      tiling: N_TILING
    }
  }
  kernels {
    name: "Q"
    id: 3
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 64
      M: 128
      K: 8192
      N: 2048
      input_tensor_size: 33554432.0
      weight_tensor_size: 134217730.0
      output_tensor_size: 33554432.0
      tiling: N_TILING
    }
  }
  kernels {
    name: "K"
    id: 4
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 64
      M: 128
      K: 8192
      N: 2048
      input_tensor_size: 33554432.0
      weight_tensor_size: 134217730.0
      output_tensor_size: 33554432.0
      tiling: N_TILING
    }
  }
  kernels {
    name: "V"
    id: 5
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 64
      M: 128
      K: 8192
      N: 2048
      input_tensor_size: 33554432.0
      weight_tensor_size: 134217730.0
      output_tensor_size: 33554432.0
      tiling: N_TILING
    }
  }
  kernels {
    name: "MHA_GEMM_1"
    id: 6
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 64
      M: 2048
      K: 128
      N: 2048
      input_tensor_1_size: 33554432.0
      input_tensor_2_size: 33554432.0
      output_tensor_size: 536870900.0
      tiling: N_TILING
    }
  }
  kernels {
    name: "SOFTMAX"
    id: 7
    fwd_bwd: FWD
    type: SIMD
    elementwise_input1 {
      outer: 64
      M: 2048
      N: 2048
      input_tensor_size: 536870900.0
      output_tensor_size: 536870900.0
      tiling: N_TILING
    }
  }
  kernels {
    name: "DropOut_1"
    id: 8
    fwd_bwd: FWD
    type: SIMD
    elementwise_input1 {
      outer: 64
      M: 2048
      N: 2048
      input_tensor_size: 536870900.0
      output_tensor_size: 536870900.0
      tiling: N_TILING
    }
  }
  kernels {
    name: "MHA_GEMM_2"
    id: 9
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_input2 {
      outer: 64
      M: 128
      K: 2048
      N: 2048
      input_tensor_1_size: 33554432.0
      input_tensor_2_size: 536870900.0
      output_tensor_size: 33554432.0
      tiling: N_TILING
    }
  }
  kernels {
    name: "PROJ_GEMM"
    id: 10
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 8192
      K: 8192
      N: 2048
      input_tensor_size: 33554432.0
      weight_tensor_size: 134217730.0
      output_tensor_size: 33554432.0
      tiling: N_TILING
    }
  }
  kernels {
    name: "DropOut_2"
    id: 11
    fwd_bwd: FWD
    type: SIMD
    elementwise_input1 {
      outer: 1
      M: 8192
      N: 2048
      input_tensor_size: 33554432.0
      output_tensor_size: 33554432.0
      tiling: N_TILING
    }
  }
  kernels {
    name: "Add_1"
    id: 12
    config: 1
    fwd_bwd: FWD
    type: SIMD
    elementwise_input1_input2 {
      outer: 1
      M: 8192
      N: 2048
      input_tensor_1_size: 33554432.0
      output_tensor_size: 33554432.0
      tiling: N_TILING
      input_tensor_2_size: 33554432.0
    }
  }
  kernels {
    name: "LayerNorm_2"
    id: 13
    config: 1
    fwd_bwd: FWD
    type: SIMD
    elementwise_input1 {
      outer: 1
      M: 8192
      N: 2048
      input_tensor_size: 33554432.0
      output_tensor_size: 33554432.0
      tiling: N_TILING
    }
  }
  kernels {
    name: "FFN0"
    id: 14
    config: 1
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 32768
      K: 8192
      N: 2048
      input_tensor_size: 33554432.0
      weight_tensor_size: 536870900.0
      output_tensor_size: 134217730.0
      tiling: N_TILING
    }
  }
  kernels {
    name: "GeLU"
    id: 15
    config: 1
    fwd_bwd: FWD
    type: SIMD
    elementwise_input1 {
      outer: 1
      M: 32768
      N: 2048
      input_tensor_size: 134217730.0
      output_tensor_size: 134217730.0
      tiling: N_TILING
    }
  }
  kernels {
    name: "FFN1"
    id: 16
    config: 1
    fwd_bwd: FWD
    type: SYSTOLIC
    gemm_input1_weight {
      outer: 1
      M: 8192
      K: 32768
      N: 2048
      input_tensor_size: 134217730.0
      weight_tensor_size: 536870900.0
      output_tensor_size: 33554432.0
      tiling: N_TILING
    }
  }
  kernels {
    name: "DropOut_3"
    id: 17
    config: 1
    fwd_bwd: FWD
    type: SIMD
    elementwise_input1 {
      outer: 1
      M: 8192
      N: 2048
      input_tensor_size: 33554432.0
      output_tensor_size: 33554432.0
      tiling: N_TILING
    }
  }
  kernels {
    name: "Add_2"
    id: 18
    config: 1
    fwd_bwd: FWD
    type: SIMD
    elementwise_input1_input2 {
      outer: 1
      M: 8192
      N: 2048
      input_tensor_1_size: 33554432.0
      output_tensor_size: 33554432.0
      tiling: N_TILING
      input_tensor_2_size: 33554432.0
    }
  }
  connections {
    startIdx: 1
    endIdx: 2
    id: 1
    lane_stage_type: LANE
  }
  connections {
    startIdx: 2
    endIdx: 3
    id: 2
    lane_stage_type: LANE
  }
  connections {
    startIdx: 2
    endIdx: 4
    id: 3
    lane_stage_type: LANE
  }
  connections {
    startIdx: 2
    endIdx: 5
    id: 4
    lane_stage_type: LANE
  }
  connections {
    startIdx: 3
    endIdx: 6
    id: 5
    lane_stage_type: LANE
  }
  connections {
    startIdx: 4
    endIdx: 6
    id: 6
    lane_stage_type: LANE
  }
  connections {
    startIdx: 6
    endIdx: 7
    id: 7
    lane_stage_type: LANE
  }
  connections {
    startIdx: 7
    endIdx: 8
    id: 8
    lane_stage_type: LANE
  }
  connections {
    startIdx: 5
    endIdx: 9
    id: 9
    lane_stage_type: LANE
  }
  connections {
    startIdx: 8
    endIdx: 9
    id: 10
    lane_stage_type: LANE
  }
  connections {
    startIdx: 9
    endIdx: 10
    id: 11
    lane_stage_type: LANE
  }
  connections {
    startIdx: 10
    endIdx: 11
    id: 12
    lane_stage_type: LANE
  }
  connections {
    startIdx: 11
    endIdx: 12
    id: 13
    lane_stage_type: LANE
  }
  connections {
    startIdx: 1
    endIdx: 12
    id: 14
    lane_stage_type: LANE
  }
  connections {
    startIdx: 12
    endIdx: 13
    id: 15
    lane_stage_type: LANE
  }
  connections {
    startIdx: 13
    endIdx: 14
    id: 16
    lane_stage_type: LANE
  }
  connections {
    startIdx: 14
    endIdx: 15
    id: 17
    lane_stage_type: LANE
  }
  connections {
    startIdx: 15
    endIdx: 16
    id: 18
    lane_stage_type: LANE
  }
  connections {
    startIdx: 16
    endIdx: 17
    id: 19
    lane_stage_type: LANE
  }
  connections {
    startIdx: 17
    endIdx: 18
    id: 20
    lane_stage_type: LANE
  }
  connections {
    startIdx: 12
    endIdx: 18
    id: 21
    lane_stage_type: LANE
  }
}
system {
  num_chip: 8
  accelerator {
    core: 1040
    systolic_width: 32
    systolic_height: 6
    sram_cap: 545259500.0
    pmu: 1040
    pmu_cap: 524288.0
    x: 40
    y: 52
    freq: 1.6
    num_switch: 2080
    placement: "rowwise"
  }
  fc_fc {
    x: 4
    y: 2
    link_bw_x: 10.0
    link_bw_y: 10.0
    par_x: "TP"
    par_y: "PP"
  }
  memory {
    dram_bw: 1638.0
    dram_cap: 68719480000.0
  }
}
cost {
  link_unit_price: 2.0
  switch_unit_price: 24.0
  dram_unit_price: 1.0
  accelerator_price: 16522.25
  link_unit_power_x: 0.052
  switch_unit_power: 0.052
  dram_unit_power: 0.16248
  accelerator_power: 444.7062
}
execution {
  llm {
    hidden_dim: 8192
    head_dim: 128
    num_head: 64
    seq_len: 2048
    num_layer: 48
    global_batch_size: 128
    micro_batch_size: 1
    tile_size: 32
    num_layer_in_graph: 1
  }
  execution_style: DATAFLOW
  num_config: 2
  perfect_overlap: true
  word: 2
}
gurobi {
  thread: 144
  gap: 0.001
  time: 3600
}
